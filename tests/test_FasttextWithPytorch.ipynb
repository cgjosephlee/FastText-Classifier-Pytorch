{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a46038ad-7742-47eb-9fc3-ce80a6b612c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-29T05:57:16.588407Z",
     "iopub.status.busy": "2022-08-29T05:57:16.588168Z",
     "iopub.status.idle": "2022-08-29T05:57:23.927184Z",
     "shell.execute_reply": "2022-08-29T05:57:23.926572Z",
     "shell.execute_reply.started": "2022-08-29T05:57:16.588379Z"
    },
    "tags": []
   },
   "source": [
    "!apt -qq update\n",
    "!apt -qq install -y build-essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3aa1aa4-3955-42d9-9963-e60fa47d6a8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T02:08:22.464753Z",
     "iopub.status.busy": "2022-09-02T02:08:22.464388Z",
     "iopub.status.idle": "2022-09-02T02:08:26.361346Z",
     "shell.execute_reply": "2022-09-02T02:08:26.360537Z",
     "shell.execute_reply.started": "2022-09-02T02:08:22.464717Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q fasttext jieba_hant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a051766-7e1c-47de-a459-17bb6acdcbde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T02:08:26.363087Z",
     "iopub.status.busy": "2022-09-02T02:08:26.362840Z",
     "iopub.status.idle": "2022-09-02T02:08:32.998599Z",
     "shell.execute_reply": "2022-09-02T02:08:32.997849Z",
     "shell.execute_reply.started": "2022-09-02T02:08:26.363051Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U gensim datasets  # pytorch-nlp torchtext torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab028c1-9108-4415-81f9-b25a67871b2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T02:08:33.000650Z",
     "iopub.status.busy": "2022-09-02T02:08:33.000334Z",
     "iopub.status.idle": "2022-09-02T02:08:35.048575Z",
     "shell.execute_reply": "2022-09-02T02:08:35.047778Z",
     "shell.execute_reply.started": "2022-09-02T02:08:33.000610Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U tqdm pysnooper"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8f75ee8-8030-49ee-af2b-b63d6e289292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T02:12:57.128578Z",
     "iopub.status.busy": "2022-08-30T02:12:57.128330Z",
     "iopub.status.idle": "2022-08-30T02:12:57.997085Z",
     "shell.execute_reply": "2022-08-30T02:12:57.996315Z",
     "shell.execute_reply.started": "2022-08-30T02:12:57.128542Z"
    },
    "tags": []
   },
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f23780ab-4547-43a3-a71b-fa460a6beb4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-26T03:14:56.995851Z",
     "iopub.status.busy": "2022-08-26T03:14:56.989652Z",
     "iopub.status.idle": "2022-08-26T03:14:59.082630Z",
     "shell.execute_reply": "2022-08-26T03:14:59.081152Z",
     "shell.execute_reply.started": "2022-08-26T03:14:56.995784Z"
    },
    "tags": []
   },
   "source": [
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51a4b91c-66e5-4449-86c5-8d9ef4823fae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T07:43:07.779815Z",
     "iopub.status.busy": "2022-09-02T07:43:07.779288Z",
     "iopub.status.idle": "2022-09-02T07:43:07.804385Z",
     "shell.execute_reply": "2022-09-02T07:43:07.803581Z",
     "shell.execute_reply.started": "2022-09-02T07:43:07.779730Z"
    },
    "tags": []
   },
   "source": [
    "import pysnooper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724dda1b-e375-442c-97d1-fef53a1297c3",
   "metadata": {},
   "source": [
    "# FastText Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2afa6d82-d963-4c88-9763-308d836e3a0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T09:41:28.391232Z",
     "iopub.status.busy": "2022-09-02T09:41:28.390967Z",
     "iopub.status.idle": "2022-09-02T09:41:30.149372Z",
     "shell.execute_reply": "2022-09-02T09:41:30.148488Z",
     "shell.execute_reply.started": "2022-09-02T09:41:28.391202Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from gensim.models.fasttext_inner import (\n",
    "    compute_ngrams,\n",
    "    compute_ngrams_bytes,\n",
    "    ft_hash_bytes,\n",
    ")\n",
    "import torch\n",
    "import pickle\n",
    "# from collections.abc import Iterable  # python >= 3.9\n",
    "from typing import Iterable  # python < 3.9\n",
    "from typing import Union, Optional, List, Dict\n",
    "\n",
    "DEFAULT_RESERVED_TOKENS = ['<pad>', '<unk>', '</s>']\n",
    "DEFAULT_PAD_INDEX = 0\n",
    "DEFAULT_UNK_INDEX = 1\n",
    "DEFAULT_EOS_INDEX = 2\n",
    "\n",
    "def _tokenize(s):\n",
    "    return s.split()\n",
    "\n",
    "def _detokenize(t):\n",
    "    return ' '.join(t)\n",
    "\n",
    "class FastTextEncoder():\n",
    "    \"\"\"\n",
    "    Implement char ngram and word ngram.\n",
    "\n",
    "    Differences:\n",
    "    - no EOS\n",
    "    - do not shrink vocab when reach (0.75 * max_vocab_size)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            texts: Union[Iterable[List[str]], List[List[str]]],\n",
    "            min_count=1,\n",
    "            max_vocab_size=None,\n",
    "            min_n=0,\n",
    "            max_n=0,\n",
    "            word_ngrams=1,\n",
    "            bucket=2000000):\n",
    "        \"\"\"\n",
    "        Build tokenizer. Require segmented sentences.\n",
    "        \"\"\"\n",
    "        # defaults\n",
    "        self.reserved_tokens = DEFAULT_RESERVED_TOKENS\n",
    "        self.pad_index = DEFAULT_PAD_INDEX\n",
    "        self.unk_index = DEFAULT_UNK_INDEX\n",
    "        self.eos_index = DEFAULT_EOS_INDEX\n",
    "        \n",
    "        self.min_count = min_count\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_n = min_n\n",
    "        self.max_n = max_n\n",
    "        self.word_ngrams = word_ngrams\n",
    "        self.bucket = bucket\n",
    "        \n",
    "        self.tokens = Counter()\n",
    "\n",
    "        for n, sequence in enumerate(texts):\n",
    "            self.tokens.update([x.strip() for x in sequence if x.strip() not in (\"\", None)])\n",
    "        \n",
    "        self.corpus_count = n + 1\n",
    "        self.corpus_total_words = sum(self.tokens.values())\n",
    "        \n",
    "        self.index_to_token = self.reserved_tokens.copy()\n",
    "        self.token_to_index = {token: index for index, token in enumerate(self.reserved_tokens)}\n",
    "        for token, count in self.tokens.items():\n",
    "            if count >= self.min_count:\n",
    "                self.index_to_token.append(token)\n",
    "                self.token_to_index[token] = len(self.index_to_token) - 1\n",
    "        \n",
    "        self.initNgrams()\n",
    "        \n",
    "        # release memory\n",
    "        self.tokens = Counter()\n",
    "    \n",
    "    @property\n",
    "    def vocab(self) -> List:\n",
    "        return self.index_to_token\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.index_to_token)\n",
    "    \n",
    "    def initNgrams(self):\n",
    "        \"\"\"\n",
    "        Initialize char ngrams for all vocabularies.\n",
    "        \"\"\"\n",
    "        self.index_to_ngram = [[]] * len(self.index_to_token)  # caveats: all items are actually same object\n",
    "        if self.max_n >= self.min_n and self.min_n > 0 and self.bucket > 0:\n",
    "            for n, v in enumerate(self.index_to_token):\n",
    "                # exclude preserved words\n",
    "                if n >= len(self.reserved_tokens):\n",
    "                    self.index_to_ngram[n] = self._compute_ngram_hashes(v, self.min_n, self.max_n, self.bucket)\n",
    "    \n",
    "    def decode(self, ids: List[int]) -> List[str]:\n",
    "        vector = [self.index_to_token[i] for i in ids]\n",
    "        return vector\n",
    "    \n",
    "    def encode(\n",
    "            self,\n",
    "            sequence: List[str],\n",
    "            append_eos: bool=False,\n",
    "            unk_to_zero: bool=False,\n",
    "            remove_unk: bool=False) -> List[int]:\n",
    "        vector = [self.token_to_index.get(token, self.unk_index) for token in sequence]\n",
    "        if append_eos:\n",
    "            vector.append(self.eos_index)\n",
    "        if unk_to_zero:\n",
    "            vector = [self.pad_index if x == self.unk_index else x for x in vector]\n",
    "        if remove_unk:\n",
    "            # but we will need correct length to do ngram hashing\n",
    "            vector = [x for x in vector if x != self.unk_index]\n",
    "        return vector\n",
    "    \n",
    "    def encode_ngram(self, sequence: List[str], input_ids: List[int]) -> List[int]:\n",
    "        # char ngram\n",
    "        char_ngrams = []\n",
    "        for w, i in zip(sequence, input_ids):\n",
    "            if i != self.eos_index:\n",
    "                if i != self.unk_index:\n",
    "                    char_ngrams += self.index_to_ngram[i]\n",
    "                else:\n",
    "                    # oov\n",
    "                    char_ngrams += self._compute_ngram_hashes(w, self.min_n, self.max_n, self.bucket)\n",
    "        \n",
    "        # word ngram\n",
    "        # we do not care oov, just hash it\n",
    "        hashes = [self._hash(x.encode(\"UTF-8\")) for x in sequence]\n",
    "        word_ngrams = self._compute_wordNgram_hashes(hashes, self.word_ngrams, self.bucket)\n",
    "        return char_ngrams + word_ngrams\n",
    "\n",
    "    def _encode_ft(self, sequence: List[str]) -> List[int]:\n",
    "        \"\"\"\n",
    "        FastText-like output.\n",
    "        Ignore UNK. Ngram id goes after end of wid.\n",
    "        \"\"\"\n",
    "        list_wids = []\n",
    "        list_hashes = []\n",
    "        list_ngrams = []  # here we separate this out\n",
    "        for w in sequence:\n",
    "            wid = self.token_to_index.get(w, self.unk_index)\n",
    "            h = self._hash(w.encode(\"UTF-8\"))\n",
    "            list_hashes.append(h)\n",
    "            if wid != self.unk_index:\n",
    "                list_wids.append(wid)\n",
    "                list_ngrams += self.index_to_ngram[wid]\n",
    "            else:\n",
    "                # oov\n",
    "                list_ngrams += self._compute_ngram_hashes(w, self.min_n, self.max_n, self.bucket)\n",
    "        # word ngrams\n",
    "        list_ngrams += self._compute_wordNgram_hashes(list_hashes, self.word_ngrams, self.bucket)\n",
    "        \n",
    "        list_ngrams = [x + self.vocab_size for x in list_ngrams]\n",
    "        return list_wids + list_ngrams\n",
    "    \n",
    "    def _batch_encode(\n",
    "            self,\n",
    "            texts: Union[Iterable[List[str]], List[List[str]]],\n",
    "            # padding=False,\n",
    "            # truncation=False,\n",
    "            # max_length=None,\n",
    "            return_tensors: Optional[str]=None,\n",
    "            unk_to_zero: bool=False) -> Dict:\n",
    "        \"\"\"\n",
    "        Separate wid and ngram.\n",
    "        \"\"\"\n",
    "        input_ids = [self.encode(x, unk_to_zero=unk_to_zero) for x in texts]\n",
    "        input_ngrams = [self.encode_ngram(x, y) for x, y in zip(texts, input_ids)] ### what if unk == 0???\n",
    "        \n",
    "        if return_tensors == \"pt\":\n",
    "            input_ids = self._pad_sequence_pt(input_ids)\n",
    "            input_ngrams = self._pad_sequence_pt(input_ngrams)\n",
    "        \n",
    "        output = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"input_ngrams\": input_ngrams\n",
    "        }\n",
    "        return output\n",
    "    \n",
    "    def _batch_encode_ft(\n",
    "            self,\n",
    "            texts: Union[Iterable[List[str]], List[List[str]]],\n",
    "            # padding=False,\n",
    "            # truncation=False,\n",
    "            # max_length=None,\n",
    "            return_tensors: Optional[str]=None) -> Dict:\n",
    "        input_ids = [self._encode_ft(x) for x in texts]\n",
    "        len_ids = [len(x) for x in input_ids]\n",
    "        if return_tensors == \"pt\":\n",
    "            input_ids = self._pad_sequence_pt(input_ids)\n",
    "            len_ids = torch.IntTensor(len_ids)\n",
    "        output = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"len_ids\": len_ids\n",
    "        }\n",
    "        return output\n",
    "    \n",
    "    def __call__(\n",
    "            self,\n",
    "            texts: Union[Iterable[List[str]], List[List[str]]],\n",
    "            ft_mode: bool=False,\n",
    "            **kwargs) -> Dict:\n",
    "        if ft_mode:\n",
    "            return self._batch_encode_ft(texts, **kwargs)\n",
    "        else:\n",
    "            return self._batch_encode(texts, **kwargs)\n",
    "    \n",
    "    def __contains__(self, word):\n",
    "        return word in self.vocab\n",
    "    \n",
    "    def get_vector(self, embedding: torch.nn.Embedding, word: str):\n",
    "        wid = self.token_to_index.get(word, self.unk_index)\n",
    "        if wid != self.unk_index:\n",
    "            return embedding.weight[wid]\n",
    "        elif self.max_n >= self.min_n and self.min_n > 0 and self.bucket > 0:\n",
    "            # oov and ngram is enabled\n",
    "            ngrams = self._compute_ngram_hashes(word, self.min_n, self.max_n, self.bucket)\n",
    "            if len(ngrams) == 0:\n",
    "                return embedding.weight[0]  # PAD and it's zeros\n",
    "            ngrams = [x + self.vocab_size for x in ngrams]\n",
    "            return torch.mean(embedding.weight[ngrams], dim=0)\n",
    "        else:\n",
    "            raise KeyError(\"cannot calculate vector for OOV word without ngrams\")\n",
    "    \n",
    "    def get_sentence_vector(self, embedding: torch.nn.Embedding, sentence: List[str]):\n",
    "        input_ids = self._encode_ft(sentence)\n",
    "        return torch.mean(embedding.weight[input_ids], dim=0)\n",
    "    \n",
    "    def save_vocab(self, fout):\n",
    "        with open(fout, \"wb\") as f:\n",
    "            pickle.dump(self.vocab, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_vocab(cls, fin, **kwargs):\n",
    "        with open(fin, \"rb\") as f:\n",
    "            vocab = pickle.load(f)\n",
    "        tokenizer = cls(vocab, **kwargs)\n",
    "        tokenizer.corpus_count = None\n",
    "        tokenizer.corpus_total_words = None\n",
    "        return tokenizer\n",
    "    \n",
    "    @classmethod\n",
    "    def _hash(cls, bytez: bytes) -> int:\n",
    "        return ft_hash_bytes(bytez)\n",
    "    \n",
    "    @classmethod\n",
    "    def _compute_ngrams(cls, word: str, min_n, max_n) -> List[str]:\n",
    "        if max_n >= min_n and min_n > 0:\n",
    "            return compute_ngrams(word, min_n, max_n)\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    @classmethod\n",
    "    def _compute_ngrams_bytes(cls, word: str, min_n, max_n) -> List[bytes]:\n",
    "        if max_n >= min_n and min_n > 0:\n",
    "            return compute_ngrams_bytes(word, min_n, max_n)\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    @classmethod\n",
    "    def _compute_ngram_hashes(cls, word: str, min_n, max_n, bucket) -> List[int]:\n",
    "        if max_n >= min_n and min_n > 0 and bucket > 0:\n",
    "            hashes = cls._compute_ngrams_bytes(word, min_n, max_n)\n",
    "            return [cls._hash(x) % bucket for x in hashes]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    @classmethod\n",
    "    def _compute_wordNgram_hashes(cls, word_hashes: List[int], word_ngrams: int, bucket: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        ref: https://github.com/facebookresearch/fastText/blob/a20c0d27cd0ee88a25ea0433b7f03038cd728459/src/dictionary.cc#L312\n",
    "        \"\"\"\n",
    "        wordNgram_hashes = []\n",
    "        if bucket > 0:\n",
    "            for i in range(len(word_hashes)):\n",
    "                h = word_hashes[i]\n",
    "                for j in range(i+1, min(len(word_hashes), i+word_ngrams)):\n",
    "                    h = h * 116049371 + word_hashes[j]\n",
    "                    wordNgram_hashes.append(h % bucket)\n",
    "        return wordNgram_hashes\n",
    "    \n",
    "    @classmethod\n",
    "    def _pad_sequence_pt(cls, seq):\n",
    "        return torch.nn.utils.rnn.pad_sequence([torch.IntTensor(x) for x in seq], batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6451fe2-d87c-4228-8a1d-1b076816de99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T09:41:30.151000Z",
     "iopub.status.busy": "2022-09-02T09:41:30.150734Z",
     "iopub.status.idle": "2022-09-02T09:41:30.162222Z",
     "shell.execute_reply": "2022-09-02T09:41:30.161485Z",
     "shell.execute_reply.started": "2022-09-02T09:41:30.150967Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class FastTextClassifierConfig():\n",
    "    vocab_size: int = 0\n",
    "    min_n: int = 2\n",
    "    max_n: int = 5\n",
    "    word_ngrams: int = 1\n",
    "    dim: int = 100\n",
    "    bucket: int = 2000000\n",
    "    lr: float = 0.1\n",
    "    lrUpdateRate: int = 100  # update lr by n tokens, here we update by batch\n",
    "    num_classes: int = 1\n",
    "    epoch: int = 5\n",
    "    batch_size: int = 256\n",
    "    \n",
    "class FastTextClassifier(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FastTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size + config.bucket, config.dim, padding_idx=0)\n",
    "        self.fc1 = nn.Linear(config.dim, config.num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        ntokens = torch.count_nonzero(input_ids)\n",
    "        output = self.embedding(input_ids)\n",
    "        output = torch.sum(output, 1) / ntokens.view([-1, 1])\n",
    "        output = self.fc1(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ce817e-71fa-45bf-91d8-8fbe8a987a49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T09:41:30.164457Z",
     "iopub.status.busy": "2022-09-02T09:41:30.163967Z",
     "iopub.status.idle": "2022-09-02T09:41:30.423692Z",
     "shell.execute_reply": "2022-09-02T09:41:30.422844Z",
     "shell.execute_reply.started": "2022-09-02T09:41:30.164419Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# @pysnooper.snoop()\n",
    "def train():\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), config.lr)\n",
    "    # scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0, total_iters=config.epoch * len(trainloader))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), config.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0, total_iters=config.epoch)\n",
    "    \n",
    "    EPOCH_BEG = 1\n",
    "    for epoch in range(EPOCH_BEG, config.epoch+1):\n",
    "        running_loss = 0.\n",
    "        for data in (pbar:=tqdm(trainloader, desc=f\"[epoch {epoch:>2}]\")):\n",
    "            # pbar.set_postfix({\"lr\": scheduler.get_last_lr()[0]})\n",
    "            labels, input_ids = data\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_ids=input_ids)\n",
    "            loss = criterion(output, labels)\n",
    "            pbar.set_postfix({\"loss\": loss.item(), \"lr\": scheduler.get_last_lr()[0]})\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()  # update by batch\n",
    "        print(f\"\"\"[epoch {epoch:>2}] train loss: {running_loss/len(trainloader):.3f} lr: {scheduler.get_last_lr()[0]:.3f}\"\"\")\n",
    "        scheduler.step()  # update by epoch\n",
    "\n",
    "def test(dataloader, disable_progress=True):\n",
    "    # model.evalute()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.\n",
    "        labels_true = []\n",
    "        labels_pred = []\n",
    "        labels_prob = []\n",
    "        for data in tqdm(dataloader, disable=disable_progress):\n",
    "            labels, input_ids = data\n",
    "            output = model(input_ids=input_ids)\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss.item()\n",
    "            output_pred = torch.max(nn.functional.softmax(output, 1), 1)\n",
    "            labels_pred += output_pred[1].tolist()\n",
    "            labels_prob += output_pred[0].tolist()\n",
    "            labels_true += labels.tolist()\n",
    "    acc = accuracy_score(labels_true, labels_pred)\n",
    "    p, r, f, _ = precision_recall_fscore_support(labels_true, labels_pred, average=\"weighted\", zero_division=0)  # WAF1-scores\n",
    "    out = {\n",
    "        \"loss\": running_loss/len(dataloader),\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": p,\n",
    "        \"recall\": r,\n",
    "        \"f1\": f\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def evalute(dataloader, disable_progress=True):\n",
    "    # model.evalute()\n",
    "    with torch.no_grad():\n",
    "        labels_pred = []\n",
    "        labels_prob = []\n",
    "        for data in tqdm(dataloader, disable=disable_progress):\n",
    "            _, input_ids = data\n",
    "            output = model(input_ids=input_ids)\n",
    "            output_pred = torch.max(nn.functional.softmax(output, 1), 1)\n",
    "            labels_pred += output_pred[1].tolist()\n",
    "            labels_prob += output_pred[0].tolist()\n",
    "    out = {\n",
    "        \"labels_pred\": labels_pred,\n",
    "        \"labels_prob\": labels_prob\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a6ce1b-87ea-44de-96dd-7832fddd3731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T09:41:30.425699Z",
     "iopub.status.busy": "2022-09-02T09:41:30.425234Z",
     "iopub.status.idle": "2022-09-02T09:41:30.436662Z",
     "shell.execute_reply": "2022-09-02T09:41:30.435861Z",
     "shell.execute_reply.started": "2022-09-02T09:41:30.425662Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list = torch.LongTensor([x[\"label\"] for x in batch])\n",
    "    out = tokenizer([_tokenize(x[\"text\"]) for x in batch], return_tensors=\"pt\", ft_mode=True)\n",
    "    input_ids = out[\"input_ids\"]\n",
    "    return label_list.to(device), input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d635309f-f2f6-4db4-90f1-04244b78d647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T09:41:30.438681Z",
     "iopub.status.busy": "2022-09-02T09:41:30.438198Z",
     "iopub.status.idle": "2022-09-02T09:41:39.252482Z",
     "shell.execute_reply": "2022-09-02T09:41:39.251530Z",
     "shell.execute_reply.started": "2022-09-02T09:41:30.438644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FastTextClassifier(\n",
       "  (embedding): Embedding(198113, 10, padding_idx=0)\n",
       "  (fc1): Linear(in_features=10, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"ag_news\"\n",
    "train_iter = load_dataset(dataset_name, split=\"train\")\n",
    "test_iter = load_dataset(dataset_name, split=\"test\")\n",
    "\n",
    "config = FastTextClassifierConfig(\n",
    "    num_classes=4,\n",
    "    batch_size=256,\n",
    "    lr=0.5,\n",
    "    min_n=2,\n",
    "    max_n=6,\n",
    "    word_ngrams=2,\n",
    "    dim=10,\n",
    "    bucket=10000\n",
    ")\n",
    "\n",
    "train_corpus = [_tokenize(x) for x in train_iter[\"text\"]]\n",
    "tokenizer = FastTextEncoder(train_corpus, min_n=config.min_n, max_n=config.max_n, word_ngrams=config.word_ngrams, bucket=config.bucket)\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trainloader = DataLoader(train_iter, batch_size=config.batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "testloader = DataLoader(test_iter, batch_size=config.batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "model = FastTextClassifier(config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a866377c-8bdd-4839-9ef9-97d5a43be332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T09:41:39.254144Z",
     "iopub.status.busy": "2022-09-02T09:41:39.253837Z",
     "iopub.status.idle": "2022-09-02T09:41:39.266299Z",
     "shell.execute_reply": "2022-09-02T09:41:39.265527Z",
     "shell.execute_reply.started": "2022-09-02T09:41:39.254107Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c7e5e57-eaff-47aa-aa1a-1572f1a37e81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T09:41:39.268060Z",
     "iopub.status.busy": "2022-09-02T09:41:39.267561Z",
     "iopub.status.idle": "2022-09-02T09:47:12.325292Z",
     "shell.execute_reply": "2022-09-02T09:47:12.324598Z",
     "shell.execute_reply.started": "2022-09-02T09:41:39.268023Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[epoch  1]:   0%|          | 0/469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-09-02 09:41:39.422 pytorch-1-10-cpu-py38-ml-t3-medium-11e7d720a60c1349ee834b037bd3:1296 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-09-02 09:41:39.614 pytorch-1-10-cpu-py38-ml-t3-medium-11e7d720a60c1349ee834b037bd3:1296 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[epoch  1]: 100%|██████████| 469/469 [01:08<00:00,  6.82it/s, loss=0.354, lr=0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch  1] train loss: 0.487 lr: 0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[epoch  2]: 100%|██████████| 469/469 [01:06<00:00,  7.06it/s, loss=0.244, lr=0.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch  2] train loss: 0.250 lr: 0.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[epoch  3]: 100%|██████████| 469/469 [01:05<00:00,  7.14it/s, loss=0.179, lr=0.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch  3] train loss: 0.199 lr: 0.300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[epoch  4]: 100%|██████████| 469/469 [01:06<00:00,  7.08it/s, loss=0.155, lr=0.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch  4] train loss: 0.164 lr: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[epoch  5]: 100%|██████████| 469/469 [01:05<00:00,  7.13it/s, loss=0.137, lr=0.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch  5] train loss: 0.141 lr: 0.100\n",
      "CPU times: user 6min 51s, sys: 12.1 s, total: 7min 3s\n",
      "Wall time: 5min 32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12ede522-0e72-4c61-a232-0d32833c5ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T09:47:12.327662Z",
     "iopub.status.busy": "2022-09-02T09:47:12.327215Z",
     "iopub.status.idle": "2022-09-02T09:47:15.128151Z",
     "shell.execute_reply": "2022-09-02T09:47:15.127377Z",
     "shell.execute_reply.started": "2022-09-02T09:47:12.327627Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.27932138641675314,\n",
       " 'accuracy': 0.9089473684210526,\n",
       " 'precision': 0.9092335200569491,\n",
       " 'recall': 0.9089473684210526,\n",
       " 'f1': 0.908725652790745}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea95f7b-c476-4fa0-919f-fe9b2c6813b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "60df901e-720c-4e4c-89b5-3b8c9d0fa587",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T04:05:34.673957Z",
     "iopub.status.busy": "2022-08-28T04:05:34.673781Z",
     "iopub.status.idle": "2022-08-28T04:05:34.683249Z",
     "shell.execute_reply": "2022-08-28T04:05:34.682721Z",
     "shell.execute_reply.started": "2022-08-28T04:05:34.673936Z"
    },
    "tags": []
   },
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e69409-b99d-4b4f-a57a-8a3f0e4c407b",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- remove `<unk>` in output? 最後平均時unk會被加進去？\n",
    "- stochastic gradient descent and a linearly decaying learning rate. 為什麼沒用？\n",
    "- Hierarchical softmax\n",
    "- ngram index 排除0 `<pad>`，加offset？\n",
    "- 吃跟ft 一樣的input format？\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf0e48-30c9-42b3-a956-b9d259a587c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd13f3ee-90dd-4112-ab6a-b13df6b8644d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T11:03:03.231463Z",
     "iopub.status.busy": "2022-09-02T11:03:03.231194Z",
     "iopub.status.idle": "2022-09-02T11:03:03.237366Z",
     "shell.execute_reply": "2022-09-02T11:03:03.236509Z",
     "shell.execute_reply.started": "2022-09-02T11:03:03.231436Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb = nn.Embedding.from_pretrained(model.embedding.weight, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ff70b6f-ee71-408b-8e08-b00eed717a51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T11:03:06.643418Z",
     "iopub.status.busy": "2022-09-02T11:03:06.643172Z",
     "iopub.status.idle": "2022-09-02T11:03:06.650933Z",
     "shell.execute_reply": "2022-09-02T11:03:06.650098Z",
     "shell.execute_reply.started": "2022-09-02T11:03:06.643392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(198113, 10, padding_idx=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7348402-10f0-4eef-ae78-aea6cd339a2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T11:01:50.901180Z",
     "iopub.status.busy": "2022-09-02T11:01:50.900932Z",
     "iopub.status.idle": "2022-09-02T11:01:50.917434Z",
     "shell.execute_reply": "2022-09-02T11:01:50.916713Z",
     "shell.execute_reply.started": "2022-09-02T11:01:50.901155Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([198113, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51569d-20c9-4f81-94e9-1dd1240f6eba",
   "metadata": {},
   "source": [
    "# FastText original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5c3ecf-55b4-4ab5-8c80-5ec49d56eb37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T02:38:56.746701Z",
     "iopub.status.busy": "2022-08-30T02:38:56.746448Z",
     "iopub.status.idle": "2022-08-30T02:38:56.856661Z",
     "shell.execute_reply": "2022-08-30T02:38:56.855800Z",
     "shell.execute_reply.started": "2022-08-30T02:38:56.746670Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79636744-b18d-4652-aa97-422566ff3569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T02:38:56.880922Z",
     "iopub.status.busy": "2022-08-30T02:38:56.880688Z",
     "iopub.status.idle": "2022-08-30T02:39:07.549322Z",
     "shell.execute_reply": "2022-08-30T02:39:07.548642Z",
     "shell.execute_reply.started": "2022-08-30T02:38:56.880897Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"ag_news\"\n",
    "train_iter = load_dataset(dataset_name, split=\"train\")\n",
    "test_iter = load_dataset(dataset_name, split=\"test\")\n",
    "\n",
    "def _to_file(it, file):\n",
    "    with open(file, \"w\") as f:\n",
    "        for i in it:\n",
    "            print(f\"\"\"__label__{i[\"label\"]} {i[\"text\"]}\"\"\", file=f)\n",
    "\n",
    "_to_file(train_iter, \"ft_train.txt\")\n",
    "_to_file(test_iter, \"ft_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "437c654f-6378-4fdf-9441-796e54893e9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T02:40:02.922427Z",
     "iopub.status.busy": "2022-08-30T02:40:02.922158Z",
     "iopub.status.idle": "2022-08-30T02:40:18.629260Z",
     "shell.execute_reply": "2022-08-30T02:40:18.628476Z",
     "shell.execute_reply.started": "2022-08-30T02:40:02.922400Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 s, sys: 172 ms, total: 14.7 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = fasttext.train_supervised(\n",
    "    input=\"ft_train.txt\",\n",
    "    dim=10,\n",
    "    epoch=5,\n",
    "    wordNgrams=2,\n",
    "    minn=2,\n",
    "    maxn=6,\n",
    "    lr=0.5,\n",
    "    bucket=10000,\n",
    "    # autotuneValidationFile=\"labels.test.txt\",\n",
    "    # autotuneDuration=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5394f1d8-b310-42cd-adbd-2dc1940adc6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T02:40:18.631083Z",
     "iopub.status.busy": "2022-08-30T02:40:18.630604Z",
     "iopub.status.idle": "2022-08-30T02:40:18.780905Z",
     "shell.execute_reply": "2022-08-30T02:40:18.780037Z",
     "shell.execute_reply.started": "2022-08-30T02:40:18.631041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7600, 0.9030263157894737, 0.9030263157894737)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"ft_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf9bca-32e8-4b0c-bca3-d804b3e7d0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-1:102112518831:image/pytorch-1.10-cpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
